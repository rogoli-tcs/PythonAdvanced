{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install googledrivedownloader\n",
    "# from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "\n",
    "# gdd.download_file_from_google_drive(file_id='1d_93d9oFNRBK9Vg6BRxs9wvRbKtNTylY',\n",
    "#                                     dest_path='content/pneumonia_dataset.zip',\n",
    "#                                     unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!apt-get update\n",
    "#!apt-get install ffmpeg libsm6 libxext6 -y\n",
    "#%pip install pandas numpy opencv-python scikit-learn keras\n",
    "import pandas as pd                                     # Data analysis and manipultion tool\n",
    "import numpy as np                                      # Fundamental package for linear algebra and multidimensional arrays\n",
    "import tensorflow as tf                                 # Deep Learning Tool\n",
    "import os                                               # OS module in Python provides a way of using operating system dependent functionality\n",
    "import cv2                                              # Library for image processing\n",
    "from sklearn.model_selection import train_test_split    # For splitting the data into train and validation set\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import callbacks\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "import gc\n",
    "from keras.metrics import binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "img_size = 100\n",
    "def create_data():\n",
    "        for item in ['normal','pneumonia']:\n",
    "            path='./content/pneumonia_dataset/train/' + item+\"/\"\n",
    "            \n",
    "            for img in os.listdir(path):         # os.listdir gets you all the list of name of files located in the given path\n",
    "                try:\n",
    "                    img_array=cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)    # converts the image to pixels and gray scales the images\n",
    "                    new_img_array=cv2.resize(img_array,(img_size,img_size))\n",
    "                    # print(img_array)\n",
    "                    if item == 'normal':\n",
    "                        data.append([new_img_array,0])\n",
    "                    else:\n",
    "                        data.append([new_img_array, 1]) # appending the list of image pixels and respective target value in data\n",
    "                except Exception as e:\n",
    "                    pass                                      # try and except is exception handling case in python, saves you from getting errors\n",
    "                \n",
    "            \n",
    "create_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7641"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([2621, 5020]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.shuffle(data)\n",
    "x = []\n",
    "y = []\n",
    "for image in data:\n",
    "  x.append(image[0])\n",
    "  y.append(image[1])\n",
    "\n",
    "# converting x & y to numpy array as they are list\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert into 4D Array\n",
    "x =  x.reshape(-1, 100, 100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(img_size=100,channels=1):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(img_size, img_size, 1)),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')    \n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def callback(tf_log_dir_name='./tf-log/',patience_lr=10):\n",
    "    cb = []\n",
    "    \"\"\"\n",
    "    Tensorboard log callback\n",
    "    \"\"\"\n",
    "    tb = callbacks.TensorBoard(log_dir=tf_log_dir_name, histogram_freq=0)\n",
    "    cb.append(tb)\n",
    "    \n",
    "#     \"\"\"\n",
    "#     Reduce Learning Rate\n",
    "#     \"\"\"\n",
    "#     reduce_lr_loss = callbacks.ReduceLROnPlateau(monitor='loss', factor=0.1, patience=patience_lr, verbose=1, min_delta=1e-4, mode='min')\n",
    "#     cb.append(reduce_lr_loss)\n",
    "    \n",
    "#     \"\"\"\n",
    "#     Early Stopping callback\n",
    "#     \"\"\"\n",
    "#     early_stop = callbacks.EarlyStopping(monitor='val_acc', min_delta=0, patience=5, verbose=1, mode='auto',save_best_only=True)\n",
    "#     cb.apppend(early_stop)\n",
    "        \n",
    "    return cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From /home/rohangoli/.local/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "382/382 - 57s - loss: 0.3976 - accuracy: 0.8130 - val_loss: 0.5837 - val_accuracy: 0.7260\n",
      "Epoch 2/10\n",
      "382/382 - 54s - loss: 0.2927 - accuracy: 0.8578 - val_loss: 0.2818 - val_accuracy: 0.8738\n",
      "Epoch 3/10\n",
      "382/382 - 52s - loss: 0.2703 - accuracy: 0.8647 - val_loss: 0.2892 - val_accuracy: 0.8607\n",
      "Epoch 4/10\n",
      "382/382 - 56s - loss: 0.2499 - accuracy: 0.8753 - val_loss: 0.4262 - val_accuracy: 0.7907\n",
      "Epoch 5/10\n",
      "382/382 - 52s - loss: 0.2452 - accuracy: 0.8793 - val_loss: 0.5341 - val_accuracy: 0.7534\n",
      "Epoch 6/10\n",
      "382/382 - 56s - loss: 0.2238 - accuracy: 0.8937 - val_loss: 0.2282 - val_accuracy: 0.8855\n",
      "Epoch 7/10\n",
      "382/382 - 53s - loss: 0.2047 - accuracy: 0.8987 - val_loss: 0.3017 - val_accuracy: 0.8443\n",
      "Epoch 8/10\n",
      "382/382 - 55s - loss: 0.2113 - accuracy: 0.8955 - val_loss: 0.2330 - val_accuracy: 0.8875\n",
      "Epoch 9/10\n",
      "382/382 - 56s - loss: 0.1966 - accuracy: 0.9067 - val_loss: 0.2792 - val_accuracy: 0.8803\n",
      "Epoch 10/10\n",
      "382/382 - 61s - loss: 0.1889 - accuracy: 0.9092 - val_loss: 0.3037 - val_accuracy: 0.8640\n",
      "accuracy: 86.40%\n",
      "Fold: 2\n",
      "Epoch 1/10\n",
      "383/383 - 55s - loss: 0.4257 - accuracy: 0.8058 - val_loss: 0.3010 - val_accuracy: 0.8390\n",
      "Epoch 2/10\n",
      "383/383 - 56s - loss: 0.2894 - accuracy: 0.8577 - val_loss: 0.3085 - val_accuracy: 0.8514\n",
      "Epoch 3/10\n",
      "383/383 - 53s - loss: 0.2562 - accuracy: 0.8731 - val_loss: 0.2796 - val_accuracy: 0.8671\n",
      "Epoch 4/10\n",
      "383/383 - 52s - loss: 0.2383 - accuracy: 0.8888 - val_loss: 0.3181 - val_accuracy: 0.8436\n",
      "Epoch 5/10\n",
      "383/383 - 53s - loss: 0.2251 - accuracy: 0.8914 - val_loss: 0.3136 - val_accuracy: 0.8325\n",
      "Epoch 6/10\n",
      "383/383 - 51s - loss: 0.2180 - accuracy: 0.8958 - val_loss: 0.2599 - val_accuracy: 0.8789\n",
      "Epoch 7/10\n",
      "383/383 - 56s - loss: 0.1966 - accuracy: 0.9074 - val_loss: 0.4540 - val_accuracy: 0.8141\n",
      "Epoch 8/10\n",
      "383/383 - 53s - loss: 0.2106 - accuracy: 0.8997 - val_loss: 0.2993 - val_accuracy: 0.8488\n",
      "Epoch 9/10\n",
      "383/383 - 55s - loss: 0.1905 - accuracy: 0.9100 - val_loss: 0.3267 - val_accuracy: 0.8423\n",
      "Epoch 10/10\n",
      "383/383 - 54s - loss: 0.1826 - accuracy: 0.9148 - val_loss: 0.3360 - val_accuracy: 0.8272\n",
      "accuracy: 82.72%\n",
      "Fold: 3\n",
      "Epoch 1/10\n",
      "383/383 - 53s - loss: 0.4254 - accuracy: 0.8091 - val_loss: 0.6922 - val_accuracy: 0.7094\n",
      "Epoch 2/10\n",
      "383/383 - 63s - loss: 0.3007 - accuracy: 0.8536 - val_loss: 0.3795 - val_accuracy: 0.7997\n",
      "Epoch 3/10\n",
      "383/383 - 52s - loss: 0.2688 - accuracy: 0.8665 - val_loss: 0.3074 - val_accuracy: 0.8370\n",
      "Epoch 4/10\n",
      "383/383 - 55s - loss: 0.2547 - accuracy: 0.8783 - val_loss: 0.4930 - val_accuracy: 0.7723\n",
      "Epoch 5/10\n",
      "383/383 - 58s - loss: 0.2430 - accuracy: 0.8817 - val_loss: 0.2924 - val_accuracy: 0.8514\n",
      "Epoch 6/10\n",
      "383/383 - 54s - loss: 0.2529 - accuracy: 0.8812 - val_loss: 0.2878 - val_accuracy: 0.8717\n",
      "Epoch 7/10\n",
      "383/383 - 53s - loss: 0.2191 - accuracy: 0.8919 - val_loss: 0.3706 - val_accuracy: 0.8213\n",
      "Epoch 8/10\n",
      "383/383 - 57s - loss: 0.2109 - accuracy: 0.9010 - val_loss: 0.2635 - val_accuracy: 0.8737\n",
      "Epoch 9/10\n",
      "383/383 - 54s - loss: 0.2048 - accuracy: 0.9023 - val_loss: 0.3513 - val_accuracy: 0.8370\n",
      "Epoch 10/10\n",
      "383/383 - 56s - loss: 0.1875 - accuracy: 0.9082 - val_loss: 0.3128 - val_accuracy: 0.8390\n",
      "accuracy: 83.90%\n",
      "Fold: 4\n",
      "Epoch 1/10\n",
      "383/383 - 58s - loss: 0.4290 - accuracy: 0.8017 - val_loss: 0.3157 - val_accuracy: 0.8501\n",
      "Epoch 2/10\n",
      "383/383 - 52s - loss: 0.3022 - accuracy: 0.8516 - val_loss: 0.4531 - val_accuracy: 0.7808\n",
      "Epoch 3/10\n",
      "383/383 - 59s - loss: 0.2733 - accuracy: 0.8642 - val_loss: 0.4547 - val_accuracy: 0.7572\n",
      "Epoch 4/10\n",
      "383/383 - 51s - loss: 0.2570 - accuracy: 0.8701 - val_loss: 0.3152 - val_accuracy: 0.8462\n",
      "Epoch 5/10\n",
      "383/383 - 51s - loss: 0.2497 - accuracy: 0.8775 - val_loss: 0.2860 - val_accuracy: 0.8763\n",
      "Epoch 6/10\n",
      "383/383 - 53s - loss: 0.2283 - accuracy: 0.8924 - val_loss: 0.3176 - val_accuracy: 0.8652\n",
      "Epoch 7/10\n",
      "383/383 - 49s - loss: 0.2196 - accuracy: 0.8943 - val_loss: 0.2882 - val_accuracy: 0.8613\n",
      "Epoch 8/10\n",
      "383/383 - 45s - loss: 0.2165 - accuracy: 0.8912 - val_loss: 0.3396 - val_accuracy: 0.8200\n",
      "Epoch 9/10\n",
      "383/383 - 46s - loss: 0.2016 - accuracy: 0.9074 - val_loss: 0.3234 - val_accuracy: 0.8397\n",
      "Epoch 10/10\n",
      "383/383 - 57s - loss: 0.1927 - accuracy: 0.9084 - val_loss: 0.3571 - val_accuracy: 0.8279\n",
      "accuracy: 82.79%\n",
      "Fold: 5\n",
      "Epoch 1/10\n",
      "383/383 - 49s - loss: 0.4046 - accuracy: 0.8192 - val_loss: 0.6789 - val_accuracy: 0.6473\n",
      "Epoch 2/10\n",
      "383/383 - 49s - loss: 0.2812 - accuracy: 0.8583 - val_loss: 0.3211 - val_accuracy: 0.8397\n",
      "Epoch 3/10\n",
      "383/383 - 58s - loss: 0.2578 - accuracy: 0.8722 - val_loss: 0.3422 - val_accuracy: 0.8521\n",
      "Epoch 4/10\n",
      "383/383 - 53s - loss: 0.2504 - accuracy: 0.8801 - val_loss: 0.6952 - val_accuracy: 0.7441\n",
      "Epoch 5/10\n",
      "383/383 - 50s - loss: 0.2406 - accuracy: 0.8827 - val_loss: 0.2887 - val_accuracy: 0.8436\n",
      "Epoch 6/10\n",
      "383/383 - 46s - loss: 0.2269 - accuracy: 0.8860 - val_loss: 0.3001 - val_accuracy: 0.8815\n",
      "Epoch 7/10\n",
      "383/383 - 46s - loss: 0.2187 - accuracy: 0.8904 - val_loss: 0.3115 - val_accuracy: 0.8495\n",
      "Epoch 8/10\n",
      "383/383 - 45s - loss: 0.2051 - accuracy: 0.9025 - val_loss: 0.4796 - val_accuracy: 0.7827\n",
      "Epoch 9/10\n",
      "383/383 - 46s - loss: 0.1998 - accuracy: 0.9035 - val_loss: 0.3445 - val_accuracy: 0.8599\n",
      "Epoch 10/10\n",
      "383/383 - 46s - loss: 0.1817 - accuracy: 0.9172 - val_loss: 0.4309 - val_accuracy: 0.8187\n",
      "accuracy: 81.87%\n",
      "Mean Accuracy: : 83.54%\n",
      "Standard Deviation: +/-: 1.57%\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cvscores=[]\n",
    "Fold=1\n",
    "for train, val in kfold.split(x,y):\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    print('Fold: {}'.format(Fold))\n",
    "    \n",
    "    X_train = x[train]\n",
    "    X_val = x[val]\n",
    "    y_train = y[train]\n",
    "    y_val = y[val]\n",
    "    \n",
    "    ## Data Normalization\n",
    "#     X_train /= 255\n",
    "#     X_val /= 255\n",
    "    \n",
    "    cb=callback()\n",
    "    \n",
    "    model=create_model(100,1)\n",
    "    \n",
    "    batch_size=16\n",
    "    epochs=10\n",
    "    model.fit(X_train, y_train, validation_data=(X_val,y_val), epochs=epochs, batch_size=batch_size, callbacks=cb, verbose=2)\n",
    "    \n",
    "    model_name = 'cnn_keras_aug_Fold_'+str(Fold)+'.h5'\n",
    "    model.save(model_name)\n",
    "    \n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(X_val, y_val, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "    \n",
    "#     # save the probability prediction of each fold in separate csv file\n",
    "#     proba = model.predict(X_test_all,batch_size=None,steps=1)\n",
    "#     labels=[np.argmax(pred) for pred in proba]\n",
    "#     keys=[get_key(path) for path in paths_test_all ]\n",
    "#     csv_name= 'submission_CNN_keras_aug_Fold'+str(Fold)+'.csv'\n",
    "#     create_submission(predictions=labels,keys=keys,path=csv_name)\n",
    "    \n",
    "    \n",
    "    Fold = Fold +1\n",
    "\n",
    "print(\"%s: %.2f%%\" % (\"Mean Accuracy: \",np.mean(cvscores)))\n",
    "print(\"%s: %.2f%%\" % (\"Standard Deviation: +/-\", np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ensemble\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 100, 100, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_14 (Sequential)      (None, 1)            2531713     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_15 (Sequential)      (None, 1)            2531713     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_16 (Sequential)      (None, 1)            2531713     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_17 (Sequential)      (None, 1)            2531713     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "average_4 (Average)             (None, 1)            0           sequential_14[0][0]              \n",
      "                                                                 sequential_15[0][0]              \n",
      "                                                                 sequential_16[0][0]              \n",
      "                                                                 sequential_17[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 10,126,852\n",
      "Trainable params: 10,125,316\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n",
      "accuracy: 92.48%\n"
     ]
    }
   ],
   "source": [
    "def ensemble(models, model_input):\n",
    "    \n",
    "    Models_output=[model(model_input) for model in models]\n",
    "    Avg = tf.keras.layers.average(Models_output)\n",
    "    \n",
    "    modelEnsemble = Model(inputs=model_input, outputs=Avg, name='ensemble')\n",
    "    modelEnsemble.summary()\n",
    "    modelEnsemble.compile(tf.keras.optimizers.Adam(lr=.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return modelEnsemble\n",
    "\n",
    "model_5 = create_model(img_size,1) \n",
    "model_2 = create_model(img_size,3) \n",
    "model_3 = create_model(img_size,3) \n",
    "model_4 = create_model(img_size,3) \n",
    "\n",
    "models = []\n",
    "\n",
    "# Load weights \n",
    "model_5.load_weights('cnn_keras_aug_Fold_1.h5')\n",
    "models.append(model_5)\n",
    "\n",
    "model_2.load_weights('cnn_keras_aug_Fold_2.h5')\n",
    "models.append(model_2)\n",
    "\n",
    "model_3.load_weights('cnn_keras_aug_Fold_3.h5')\n",
    "models.append(model_3)\n",
    "\n",
    "model_4.load_weights('cnn_keras_aug_Fold_4.h5')\n",
    "models.append(model_4)\n",
    "\n",
    "model_input = tf.keras.layers.Input(shape=models[0].input_shape[1:])\n",
    "ensemble_model = ensemble(models, model_input)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "scores = ensemble_model.evaluate(X_val, y_val, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (ensemble_model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'cnn_keras_ensebmle.h5'\n",
    "ensemble_model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of image names i.e.  606 matches the number of file paths i.e.  606\n"
     ]
    }
   ],
   "source": [
    "# Loading the order of the image's name that has been provided\n",
    "test_image_order = pd.read_csv(\"./content/pneumonia_dataset/test.csv\")\n",
    "test_image_order.head()\n",
    "file_paths = [[fname, './content/pneumonia_dataset/test/' + fname] for fname in test_image_order['filename']]\n",
    "# Confirm if number of images is same as number of labels given\n",
    "if len(test_image_order) == len(file_paths):\n",
    "    print('Number of image names i.e. ', len(test_image_order), 'matches the number of file paths i.e. ', len(file_paths))\n",
    "else:\n",
    "    print('Number of image names does not match the number of filepaths')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = pd.DataFrame(file_paths, columns=['filename', 'filepaths'])\n",
    "test_images.head()\n",
    "test_pixel_data = []     # initialize an empty numpy array\n",
    "for i in range(len(test_images)):\n",
    "  \n",
    "  img_array = cv2.imread(test_images['filepaths'][i], cv2.IMREAD_GRAYSCALE)   # converting the image to gray scale\n",
    "  new_img_array=cv2.resize(img_array,(img_size,img_size))\n",
    "  test_pixel_data.append(new_img_array)\n",
    "test_pixel_data = np.asarray(test_pixel_data)\n",
    "test_pixel_data =  test_pixel_data.reshape(-1, 100, 100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = ensemble_model.predict(test_pixel_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for item in pred:\n",
    "  if item <= 0.5:\n",
    "    predictions.append('normal')\n",
    "  else:\n",
    "    predictions.append('pneumonia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame({'filename': test_images['filename'], 'label': predictions})  # prediction is nothing but the final predictions of your model on input features of your new unseen test data\n",
    "res.to_csv(\"submission.csv\", index = False)      # the csv file will be saved locally on the same location where this notebook is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
