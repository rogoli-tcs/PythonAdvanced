{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install googledrivedownloader\n",
    "# from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "\n",
    "# gdd.download_file_from_google_drive(file_id='1d_93d9oFNRBK9Vg6BRxs9wvRbKtNTylY',\n",
    "#                                     dest_path='content/pneumonia_dataset.zip',\n",
    "#                                     unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!apt-get update\n",
    "#!apt-get install ffmpeg libsm6 libxext6 -y\n",
    "#%pip install pandas numpy opencv-python scikit-learn keras\n",
    "import pandas as pd                                     # Data analysis and manipultion tool\n",
    "import numpy as np                                      # Fundamental package for linear algebra and multidimensional arrays\n",
    "import tensorflow as tf                                 # Deep Learning Tool\n",
    "import os                                               # OS module in Python provides a way of using operating system dependent functionality\n",
    "import cv2                                              # Library for image processing\n",
    "from sklearn.model_selection import train_test_split    # For splitting the data into train and validation set\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import callbacks\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "import gc\n",
    "from keras.metrics import binary_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "img_size = 100\n",
    "def create_data():\n",
    "        for item in ['normal','pneumonia']:\n",
    "            path='./content/pneumonia_dataset/train/' + item+\"/\"\n",
    "            \n",
    "            for img in os.listdir(path):         # os.listdir gets you all the list of name of files located in the given path\n",
    "                try:\n",
    "                    img_array=cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)    # converts the image to pixels and gray scales the images\n",
    "                    norm_image = cv2.normalize(img_array, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "                    new_img_array=cv2.resize(norm_image,(img_size,img_size))\n",
    "                    if item == 'normal':\n",
    "                        data.append([new_img_array,0])\n",
    "                    else:\n",
    "                        data.append([new_img_array, 1]) # appending the list of image pixels and respective target value in data\n",
    "                except Exception as e:\n",
    "                    pass                                      # try and except is exception handling case in python, saves you from getting errors\n",
    "                \n",
    "            \n",
    "create_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2425"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([1280, 1145]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.shuffle(data)\n",
    "x = []\n",
    "y = []\n",
    "for image in data:\n",
    "  x.append(image[0])\n",
    "  y.append(image[1])\n",
    "\n",
    "# converting x & y to numpy array as they are list\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert into 4D Array\n",
    "x =  x.reshape(-1, 100, 100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2425, 100, 100, 1)\n",
      "(2425,)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_aug(X_train,X_test,y_train,y_test,train_batch_size,test_batch_size):\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        rescale=1.0/255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "    test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "    train_batch = train_datagen.flow(X_train,y_train,batch_size=train_batch_size)\n",
    "    test_batch = test_datagen.flow(X_test,y_test,batch_size=test_batch_size)\n",
    "    return (train_batch,test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(img_size=100,channels=1):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(img_size, img_size, 1)),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')      \n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    #model.compile(loss='binary_crossentropy', optimizer=RMSprop(lr=0.1), metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def callback(tf_log_dir_name='./tf-log/',patience_lr=10):\n",
    "    cb = []\n",
    "    \"\"\"\n",
    "    Tensorboard log callback\n",
    "    \"\"\"\n",
    "    tb = callbacks.TensorBoard(log_dir=tf_log_dir_name, histogram_freq=0)\n",
    "    cb.append(tb)\n",
    "        \n",
    "    return cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Epoch 1/10\n",
      "76/76 [==============================] - 13s 169ms/step - loss: 0.7575 - accuracy: 0.6304 - val_loss: 0.7691 - val_accuracy: 0.4600\n",
      "Epoch 2/10\n",
      "76/76 [==============================] - 11s 143ms/step - loss: 0.6352 - accuracy: 0.6576 - val_loss: 1.1006 - val_accuracy: 0.4575\n",
      "Epoch 3/10\n",
      "76/76 [==============================] - 11s 148ms/step - loss: 0.6194 - accuracy: 0.6510 - val_loss: 1.0258 - val_accuracy: 0.4617\n",
      "Epoch 4/10\n",
      "76/76 [==============================] - 11s 140ms/step - loss: 0.5974 - accuracy: 0.6675 - val_loss: 0.7191 - val_accuracy: 0.5697\n",
      "Epoch 5/10\n",
      "76/76 [==============================] - 11s 142ms/step - loss: 0.5854 - accuracy: 0.7021 - val_loss: 0.6573 - val_accuracy: 0.6167\n",
      "Epoch 6/10\n",
      "76/76 [==============================] - 11s 149ms/step - loss: 0.5722 - accuracy: 0.6972 - val_loss: 0.6969 - val_accuracy: 0.5598\n",
      "Epoch 7/10\n",
      "76/76 [==============================] - 11s 140ms/step - loss: 0.5390 - accuracy: 0.7071 - val_loss: 0.7107 - val_accuracy: 0.5111\n",
      "Epoch 8/10\n",
      "76/76 [==============================] - 11s 140ms/step - loss: 0.5470 - accuracy: 0.6980 - val_loss: 0.7598 - val_accuracy: 0.5705\n",
      "Epoch 9/10\n",
      "76/76 [==============================] - 11s 141ms/step - loss: 0.5449 - accuracy: 0.7112 - val_loss: 0.6248 - val_accuracy: 0.6694\n",
      "Epoch 10/10\n",
      "76/76 [==============================] - 11s 140ms/step - loss: 0.5089 - accuracy: 0.7475 - val_loss: 0.6582 - val_accuracy: 0.6480\n",
      "accuracy: 64.80%\n",
      "Fold: 2\n",
      "Epoch 1/10\n",
      "76/76 [==============================] - 14s 185ms/step - loss: 0.7469 - accuracy: 0.6059 - val_loss: 0.6945 - val_accuracy: 0.5215\n",
      "Epoch 2/10\n",
      "76/76 [==============================] - 11s 139ms/step - loss: 0.6315 - accuracy: 0.6678 - val_loss: 0.7155 - val_accuracy: 0.5256\n",
      "Epoch 3/10\n",
      "76/76 [==============================] - 10s 136ms/step - loss: 0.5979 - accuracy: 0.6834 - val_loss: 0.8334 - val_accuracy: 0.5206\n",
      "Epoch 4/10\n",
      "76/76 [==============================] - 10s 136ms/step - loss: 0.5859 - accuracy: 0.6958 - val_loss: 0.8781 - val_accuracy: 0.5553\n",
      "Epoch 5/10\n",
      "76/76 [==============================] - 10s 136ms/step - loss: 0.5695 - accuracy: 0.7040 - val_loss: 1.1322 - val_accuracy: 0.4942\n",
      "Epoch 6/10\n",
      "76/76 [==============================] - 13s 169ms/step - loss: 0.5596 - accuracy: 0.7024 - val_loss: 0.7174 - val_accuracy: 0.5998\n",
      "Epoch 7/10\n",
      "76/76 [==============================] - 19s 248ms/step - loss: 0.5531 - accuracy: 0.7246 - val_loss: 0.6422 - val_accuracy: 0.6510\n",
      "Epoch 8/10\n",
      "76/76 [==============================] - 16s 210ms/step - loss: 0.5066 - accuracy: 0.7609 - val_loss: 0.6687 - val_accuracy: 0.6328\n",
      "Epoch 9/10\n",
      "76/76 [==============================] - 11s 139ms/step - loss: 0.4888 - accuracy: 0.7634 - val_loss: 0.7392 - val_accuracy: 0.6089\n",
      "Epoch 10/10\n",
      "76/76 [==============================] - 10s 138ms/step - loss: 0.4633 - accuracy: 0.7758 - val_loss: 0.6758 - val_accuracy: 0.6320\n",
      "accuracy: 63.20%\n",
      "Mean Accuracy: : 64.00%\n",
      "Standard Deviation: +/-: 0.80%\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=2, shuffle=True, random_state=42)\n",
    "cvscores=[]\n",
    "Fold=1\n",
    "for train, val in kfold.split(x,y):\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    print('Fold: {}'.format(Fold))\n",
    "    \n",
    "    X_train = x[train]\n",
    "    X_val = x[val]\n",
    "    y_train = y[train]\n",
    "    y_val = y[val]\n",
    "    \n",
    "    # Data Augmentation and Normalization(OPTIONAL) UNCOMMENT THIS FOR AUGMENTATION !!\n",
    "#     batch_size = 32\n",
    "#     train_batch, val_batch = data_aug(X_train,X_val,y_train,y_val, batch_size, batch_size)\n",
    "    \n",
    "    cb=callback()\n",
    "    \n",
    "    model=create_model(100,1)\n",
    "    \n",
    "    # Fit generator for Data Augmentation\n",
    "#     epochs = 10 \n",
    "#     model.fit(train_batch, validation_data=val_batch, epochs=epochs, validation_steps= X_val.shape[0] // batch_size, \n",
    "#                        steps_per_epoch= X_train.shape[0] // batch_size, callbacks=cb, verbose=1)\n",
    "\n",
    "# Fit the model for without Data Augmentation\n",
    "    batch_size=16\n",
    "    epochs=10\n",
    "    model.fit(X_train, y_train, validation_data=(X_val,y_val), epochs=epochs, batch_size=batch_size, callbacks=cb, verbose=1)\n",
    "    \n",
    "    model_name = 'cnn_keras_aug_Fold_'+str(Fold)+'.h5'\n",
    "    model.save(model_name)\n",
    "    \n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(X_val, y_val, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "    \n",
    "#     # save the probability prediction of each fold in separate csv file\n",
    "#     proba = model.predict(X_test_all,batch_size=None,steps=1)\n",
    "#     labels=[np.argmax(pred) for pred in proba]\n",
    "#     keys=[get_key(path) for path in paths_test_all ]\n",
    "#     csv_name= 'submission_CNN_keras_aug_Fold'+str(Fold)+'.csv'\n",
    "#     create_submission(predictions=labels,keys=keys,path=csv_name)\n",
    "    \n",
    "    \n",
    "    Fold = Fold +1\n",
    "\n",
    "print(\"%s: %.2f%%\" % (\"Mean Accuracy: \",np.mean(cvscores)))\n",
    "print(\"%s: %.2f%%\" % (\"Standard Deviation: +/-\", np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ensemble\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 100, 100, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 1)            2531713     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 1)            2531713     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_3 (Sequential)       (None, 1)            2531713     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_4 (Sequential)       (None, 1)            2531713     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "average (Average)               (None, 1)            0           sequential_1[0][0]               \n",
      "                                                                 sequential_2[0][0]               \n",
      "                                                                 sequential_3[0][0]               \n",
      "                                                                 sequential_4[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 10,126,852\n",
      "Trainable params: 10,125,316\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n",
      "accuracy: 53.20%\n"
     ]
    }
   ],
   "source": [
    "def ensemble(models, model_input):\n",
    "    \n",
    "    Models_output=[model(model_input) for model in models]\n",
    "    Avg = tf.keras.layers.average(Models_output)\n",
    "    \n",
    "    modelEnsemble = Model(inputs=model_input, outputs=Avg, name='ensemble')\n",
    "    modelEnsemble.summary()\n",
    "    modelEnsemble.compile(tf.keras.optimizers.Adam(lr=.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return modelEnsemble\n",
    "\n",
    "model_1 = create_model(img_size,1) \n",
    "model_2 = create_model(img_size,3) \n",
    "model_3 = create_model(img_size,3) \n",
    "model_4 = create_model(img_size,3) \n",
    "\n",
    "models = []\n",
    "\n",
    "# Load weights \n",
    "model_1.load_weights('cnn_keras_aug_Fold_1.h5')\n",
    "models.append(model_1)\n",
    "\n",
    "model_2.load_weights('cnn_keras_aug_Fold_2.h5')\n",
    "models.append(model_2)\n",
    "\n",
    "model_3.load_weights('cnn_keras_aug_Fold_3.h5')\n",
    "models.append(model_3)\n",
    "\n",
    "model_4.load_weights('cnn_keras_aug_Fold_4.h5')\n",
    "models.append(model_4)\n",
    "\n",
    "model_input = tf.keras.layers.Input(shape=models[0].input_shape[1:])\n",
    "ensemble_model = ensemble(models, model_input)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "scores = ensemble_model.evaluate(X_val, y_val, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (ensemble_model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'cnn_keras_ensebmle.h5'\n",
    "ensemble_model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of image names i.e.  606 matches the number of file paths i.e.  606\n"
     ]
    }
   ],
   "source": [
    "# Loading the order of the image's name that has been provided\n",
    "test_image_order = pd.read_csv(\"./content/pneumonia_dataset/test.csv\")\n",
    "test_image_order.head()\n",
    "file_paths = [[fname, './content/pneumonia_dataset/test/' + fname] for fname in test_image_order['filename']]\n",
    "# Confirm if number of images is same as number of labels given\n",
    "if len(test_image_order) == len(file_paths):\n",
    "    print('Number of image names i.e. ', len(test_image_order), 'matches the number of file paths i.e. ', len(file_paths))\n",
    "else:\n",
    "    print('Number of image names does not match the number of filepaths')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = pd.DataFrame(file_paths, columns=['filename', 'filepaths'])\n",
    "test_images.head()\n",
    "test_pixel_data = []     # initialize an empty numpy array\n",
    "for i in range(len(test_images)):\n",
    "  \n",
    "  img_array = cv2.imread(test_images['filepaths'][i], cv2.IMREAD_GRAYSCALE)   # converting the image to gray scale\n",
    "  new_img_array=cv2.resize(img_array,(img_size,img_size))\n",
    "  test_pixel_data.append(new_img_array)\n",
    "test_pixel_data = np.asarray(test_pixel_data)\n",
    "test_pixel_data =  test_pixel_data.reshape(-1, 100, 100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = ensemble_model.predict(test_pixel_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for item in pred:\n",
    "  if item <= 0.5:\n",
    "    predictions.append('normal')\n",
    "  else:\n",
    "    predictions.append('pneumonia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame({'filename': test_images['filename'], 'label': predictions})  # prediction is nothing but the final predictions of your model on input features of your new unseen test data\n",
    "res.to_csv(\"submission.csv\", index = False)      # the csv file will be saved locally on the same location where this notebook is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
